<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ø§ØªØµÙ„ Ø¨ÙÙ‡Ø¯ - AI Call App</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .container {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            max-width: 600px;
            width: 100%;
            padding: 30px;
        }

        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 30px;
            font-size: 28px;
        }

        .call-button {
            width: 100%;
            padding: 20px;
            font-size: 20px;
            font-weight: bold;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 12px;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            margin-bottom: 20px;
        }

        .call-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(102, 126, 234, 0.4);
        }

        .call-button:active {
            transform: translateY(0);
        }

        .call-button:disabled {
            opacity: 0.6;
            cursor: not-allowed;
        }

        .status {
            text-align: center;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 8px;
            font-weight: bold;
        }

        .status.idle {
            background: #e3f2fd;
            color: #1976d2;
        }

        .status.listening {
            background: #fff3e0;
            color: #f57c00;
        }

        .status.processing {
            background: #f3e5f5;
            color: #7b1fa2;
        }

        .status.speaking {
            background: #e8f5e9;
            color: #388e3c;
        }

        .conversation {
            max-height: 400px;
            overflow-y: auto;
            margin-bottom: 20px;
            padding: 15px;
            background: #f5f5f5;
            border-radius: 12px;
        }

        .message {
            margin-bottom: 15px;
            padding: 12px;
            border-radius: 8px;
            animation: fadeIn 0.3s;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .message.user {
            background: #667eea;
            color: white;
            text-align: left;
            margin-right: 20%;
        }

        .message.fahd {
            background: #764ba2;
            color: white;
            text-align: right;
            margin-left: 20%;
        }

        .products-info {
            background: #fff9e6;
            border: 2px solid #ffc107;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
        }

        .products-info h3 {
            color: #f57c00;
            margin-bottom: 10px;
        }

        .products-info ul {
            list-style: none;
            padding: 0;
        }

        .products-info li {
            padding: 5px 0;
            color: #333;
        }

        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top-color: white;
            animation: spin 1s ease-in-out infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        .status.error {
            background: #ffebee;
            color: #c62828;
            font-size: 18px;
            padding: 20px;
            border: 2px solid #c62828;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Ø§ØªØµÙ„ Ø¨ÙÙ‡Ø¯ - Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª</h1>
        
        <div class="products-info">
            <h3>Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:</h3>
            <ul>
                <li>ğŸ‘• Ù‚Ù…ÙŠØµ - 50 Ø±ÙŠØ§Ù„</li>
                <li>ğŸ‘– Ø¨Ù†Ø·Ù„ÙˆÙ† - 20 Ø±ÙŠØ§Ù„</li>
                <li>ğŸ•Œ Ø³Ø¬Ø§Ø¯Ø© ØµÙ„Ø§Ø© - 100 Ø±ÙŠØ§Ù„</li>
            </ul>
        </div>

        <button id="callButton" class="call-button">ğŸ“ Ø§ØªØµÙ„ Ø¨ÙÙ‡Ø¯</button>
        
        <div id="status" class="status idle">Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§ØªØµØ§Ù„</div>
        
        <div id="conversation" class="conversation"></div>
    </div>

    <script>
        const GROQ_API_KEY = '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';
        const GROQ_API_URL = 'https://api.groq.com/openai/v1';

        let isCalling = false;
        let mediaRecorder = null;
        let audioChunks = [];
        let recognition = null;
        let currentAudio = null;
        let microphoneStream = null; // Keep stream open for entire call
        let silenceTimer = null;
        let audioContext = null;
        let analyser = null;
        let silenceThreshold = 0.01; // Adjust based on testing
        let lastSoundTime = Date.now();
        let conversationHistory = []; // Store chat history for context

        const callButton = document.getElementById('callButton');
        const statusDiv = document.getElementById('status');
        const conversationDiv = document.getElementById('conversation');

        // System prompt for Fahd
        const SYSTEM_PROMPT = `Ø£Ù†Øª ÙÙ‡Ø¯ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª Ø³Ø¹ÙˆØ¯ÙŠ ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ù‡Ù†ÙŠ. Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠ Ø¥ØªÙ…Ø§Ù… Ø§Ù„Ø·Ù„Ø¨ ÙˆØ¥ØºÙ„Ø§Ù‚ Ø§Ù„ØµÙÙ‚Ø©.

Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·:
1. Ù‚Ù…ÙŠØµ - 50 Ø±ÙŠØ§Ù„
2. Ø¨Ù†Ø·Ù„ÙˆÙ† - 20 Ø±ÙŠØ§Ù„  
3. Ø³Ø¬Ø§Ø¯Ø© ØµÙ„Ø§Ø© - 100 Ø±ÙŠØ§Ù„

Ù…Ù‡Ù…ØªÙƒ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨:
1. Ø±Ø­Ø¨ Ø¨Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø©
2. Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
3. Ø¨Ø¹Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù†ØªØ¬ØŒ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø§Ø³Ù… Ø§Ù„ÙƒØ§Ù…Ù„
4. Ø¨Ø¹Ø¯ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØ§Ù…Ù„
5. Ø¨Ø¹Ø¯ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†ØŒ Ù‚Ù„: "ØªÙ… Ø¹Ù…Ù„ Ø§Ù„Ø·Ù„Ø¨ ÙˆØ³ØªØµÙ„Ùƒ Ø±Ø³Ø§Ù„Ø© Ù‚Ø±ÙŠØ¨Ø§Ù‹"

Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
- Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ø®Ø§Ø±Ø¬ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¹ (Ù…Ø«Ù„ Ø§Ù„Ø·Ù‚Ø³ØŒ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŒ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø¹Ø§Ù…Ø©)ØŒ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ ØªÙ…Ø§Ù…Ø§Ù‹ ÙˆØ§Ø±Ø¬Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø­Ø¯ÙŠØ« Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ§Ù„Ø·Ù„Ø¨
- Ù„Ø§ ØªØ±Ø¯ Ø¹Ù„Ù‰ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø© Ø®Ø§Ø±Ø¬ÙŠØ©ØŒ ÙÙ‚Ø· Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø¥ØªÙ…Ø§Ù… Ø§Ù„Ø·Ù„Ø¨
- Ø£Ø¬Ø¨ Ø¨Ø¥ÙŠØ¬Ø§Ø² Ø´Ø¯ÙŠØ¯ØŒ Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² Ø¬Ù…Ù„ØªÙŠÙ† ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ ** Ø£Ùˆ * Ø£Ùˆ _ Ø£Ùˆ Ø£ÙŠ ØªÙ†Ø³ÙŠÙ‚Ø§Øª
- Ø§ÙƒØªØ¨ Ø§Ù„Ù†Øµ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø±Ù…ÙˆØ² Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø§Øª
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰
- ÙƒÙ† Ù…Ù‡Ø°Ø¨Ø§ ÙˆÙ…Ù‡Ù†ÙŠØ§ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§
- Ø±ÙƒØ² Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø·Ù„Ø¨`;

        // Initialize Web Speech API for browser-based speech recognition (fallback)
        function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.lang = 'ar-SA';
                recognition.continuous = false;
                recognition.interimResults = false;
            }
        }

        // Speech to Text using Groq Whisper
        async function speechToText(audioBlob) {
            try {
                const formData = new FormData();
                formData.append('file', audioBlob, 'audio.webm');
                formData.append('model', 'whisper-large-v3');
                formData.append('language', 'ar');

                const response = await fetch(`${GROQ_API_URL}/audio/transcriptions`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${GROQ_API_KEY}`
                    },
                    body: formData
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    console.error('STT Error:', errorText);
                    throw new Error(`Speech to text failed: ${response.statusText}`);
                }

                const data = await response.json();
                return data.text || '';
            } catch (error) {
                console.error('Speech to text error:', error);
                throw error;
            }
        }

        // Text processing using Groq GPT
        async function processText(userMessage) {
            try {
                // Build messages array with system prompt, conversation history, and current message
                // conversationHistory already contains all previous messages (user and assistant)
                const messages = [
                    {
                        role: 'system',
                        content: SYSTEM_PROMPT
                    },
                    ...conversationHistory, // All previous conversation history
                    {
                        role: 'user',
                        content: userMessage
                    }
                ];
                
                // Try the specified model first, fallback to available models
                const models = ['openai/gpt-oss-120b', 'llama-3.1-70b-versatile', 'mixtral-8x7b-32768'];
                
                for (const model of models) {
                    try {
                        const response = await fetch(`${GROQ_API_URL}/chat/completions`, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json',
                                'Authorization': `Bearer ${GROQ_API_KEY}`
                            },
                            body: JSON.stringify({
                                model: model,
                                messages: messages,
                                temperature: 0.7,
                                max_tokens: 150
                            })
                        });

                        if (response.ok) {
                            const data = await response.json();
                            return data.choices[0].message.content;
                        }
                    } catch (e) {
                        console.log(`Model ${model} failed, trying next...`);
                        continue;
                    }
                }
                
                throw new Error('All models failed');
            } catch (error) {
                console.error('Text processing error:', error);
                // Fallback response
                return 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©ØŸ';
            }
        }

        // Text to Speech using Groq
        async function textToSpeech(text) {
            try {
                // Stop any currently playing audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                    currentAudio = null;
                }

                const response = await fetch(`${GROQ_API_URL}/audio/speech`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${GROQ_API_KEY}`
                    },
                    body: JSON.stringify({
                        model: 'canopylabs/orpheus-arabic-saudi',
                        input: text,
                        voice: 'fahad',
                        response_format: 'wav'
                    })
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    console.error('TTS Error:', errorText);
                    throw new Error(`Text to speech failed: ${response.statusText}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                currentAudio = audio;
                
                // Wait for audio to be ready and play
                return new Promise((resolve, reject) => {
                    let played = false;
                    
                    const playAudio = () => {
                        if (played) return;
                        played = true;
                        
                        audio.play().then(() => {
                            console.log('Audio playing successfully');
                        }).catch((playError) => {
                            console.error('Play error:', playError);
                            // Try fallback
                            URL.revokeObjectURL(audioUrl);
                            currentAudio = null;
                            fallbackTextToSpeech(text).then(resolve).catch(resolve);
                        });
                    };

                    // Try to play when audio can start playing
                    audio.addEventListener('canplay', () => {
                        playAudio();
                    }, { once: true });
                    
                    // Also try when data is loaded
                    audio.addEventListener('loadeddata', () => {
                        if (audio.readyState >= 2) {
                            playAudio();
                        }
                    }, { once: true });

                    audio.onended = () => {
                        URL.revokeObjectURL(audioUrl);
                        if (currentAudio === audio) {
                            currentAudio = null;
                        }
                        resolve();
                    };
                    
                    audio.onerror = (e) => {
                        console.error('Audio playback error:', e, audio.error);
                        URL.revokeObjectURL(audioUrl);
                        if (currentAudio === audio) {
                            currentAudio = null;
                        }
                        // Try fallback
                        fallbackTextToSpeech(text).then(resolve).catch(resolve);
                    };

                    // Timeout fallback after 3 seconds if audio doesn't start
                    const timeoutId = setTimeout(() => {
                        if (!played && currentAudio === audio) {
                            console.log('Audio loading timeout, using fallback');
                            audio.pause();
                            URL.revokeObjectURL(audioUrl);
                            currentAudio = null;
                            fallbackTextToSpeech(text).then(resolve).catch(resolve);
                        }
                    }, 3000);
                    
                    // Clear timeout when audio starts playing
                    audio.addEventListener('play', () => {
                        clearTimeout(timeoutId);
                    }, { once: true });
                });
            } catch (error) {
                console.error('Text to speech error:', error);
                // Fallback to browser speech synthesis
                return fallbackTextToSpeech(text);
            }
        }

        // Fallback text to speech using browser API
        function fallbackTextToSpeech(text) {
            return new Promise((resolve) => {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'ar-SA';
                utterance.onend = resolve;
                utterance.onerror = resolve;
                speechSynthesis.speak(utterance);
            });
        }

        // Add message to conversation
        function addMessage(text, sender) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}`;
            messageDiv.textContent = text;
            conversationDiv.appendChild(messageDiv);
            conversationDiv.scrollTop = conversationDiv.scrollHeight;
        }

        // Update status
        function updateStatus(text, className) {
            statusDiv.textContent = text;
            statusDiv.className = `status ${className}`;
        }

        // Detect silence and auto-stop recording
        function startSilenceDetection() {
            if (!microphoneStream || !isCalling) return;
            
            try {
                // Create audio context for silence detection
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                const source = audioContext.createMediaStreamSource(microphoneStream);
                source.connect(analyser);
                
                const dataArray = new Uint8Array(analyser.frequencyBinCount);
                lastSoundTime = Date.now();
                
                const checkSilence = () => {
                    if (!isCalling || !analyser) return;
                    
                    analyser.getByteFrequencyData(dataArray);
                    const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
                    const normalized = average / 255;
                    
                    if (normalized > silenceThreshold) {
                        // Sound detected
                        lastSoundTime = Date.now();
                    } else {
                        // Check if silence for 2 seconds
                        const silenceDuration = Date.now() - lastSoundTime;
                        if (silenceDuration >= 2000 && mediaRecorder && mediaRecorder.state === 'recording') {
                            // 2 seconds of silence - stop recording
                            console.log('2 seconds of silence detected, stopping recording');
                            stopRecording();
                            return;
                        }
                    }
                    
                    if (isCalling) {
                        requestAnimationFrame(checkSilence);
                    }
                };
                
                checkSilence();
            } catch (error) {
                console.error('Error setting up silence detection:', error);
            }
        }

        // Start recording using existing stream
        function startRecording() {
            if (!microphoneStream) {
                console.error('No microphone stream available');
                return;
            }
            
            try {
                // Stop previous recorder if exists
                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }
                
                // Clear any existing silence timer
                if (silenceTimer) {
                    clearTimeout(silenceTimer);
                    silenceTimer = null;
                }
                
                mediaRecorder = new MediaRecorder(microphoneStream, {
                    mimeType: 'audio/webm'
                });

                audioChunks = [];
                lastSoundTime = Date.now();

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = async () => {
                    // Clean up audio context
                    if (audioContext) {
                        audioContext.close();
                        audioContext = null;
                        analyser = null;
                    }
                    
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    if (audioChunks.length > 0) {
                        await processAudio(audioBlob);
                    }
                };

                mediaRecorder.start();
                updateStatus('ğŸ¤ Ø£Ø³ØªÙ…Ø¹ Ø¥Ù„ÙŠÙƒ...', 'listening');
                
                // Start silence detection
                startSilenceDetection();
            } catch (error) {
                console.error('Error starting recording:', error);
                console.error('Error name:', error.name);
                console.error('Error message:', error.message);
                console.error('Error stack:', error.stack);
                updateStatus(`âŒ Recording Error: ${error.name} - ${error.message}`, 'error');
            }
        }

        // Stop recording
        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
        }

        // Process audio: STT -> GPT -> TTS
        async function processAudio(audioBlob) {
            try {
                updateStatus('ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª...', 'processing');
                
                // Speech to Text
                const userText = await speechToText(audioBlob);
                addMessage(userText, 'user');

                updateStatus('ğŸ¤– ÙÙ‡Ø¯ ÙŠÙÙƒØ±...', 'processing');

                // Text processing (includes conversation history)
                let fahdResponse = await processText(userText);
                
                // Clean response - remove any markdown symbols
                fahdResponse = fahdResponse
                    .replace(/\*\*/g, '')
                    .replace(/\*/g, '')
                    .replace(/_/g, '')
                    .replace(/`/g, '')
                    .replace(/#/g, '')
                    .trim();
                
                // Add both user message and assistant response to history
                conversationHistory.push({
                    role: 'user',
                    content: userText
                });
                conversationHistory.push({
                    role: 'assistant',
                    content: fahdResponse
                });
                
                // Display Fahd's response in UI
                addMessage(fahdResponse, 'fahd');

                updateStatus('ğŸ”Š ÙÙ‡Ø¯ ÙŠØªØ­Ø¯Ø«...', 'speaking');

                // Text to Speech
                await textToSpeech(fahdResponse);

                // Ready for next input
                if (isCalling) {
                    updateStatus('ğŸ¤ Ø£Ø³ØªÙ…Ø¹ Ø¥Ù„ÙŠÙƒ...', 'listening');
                    // Continue listening - reuse existing stream
                    startRecording();
                }
            } catch (error) {
                console.error('Processing error:', error);
                console.error('Error name:', error.name);
                console.error('Error message:', error.message);
                console.error('Error stack:', error.stack);
                updateStatus(`âŒ Processing Error: ${error.name} - ${error.message}`, 'error');
                addMessage(`Error: ${error.message}`, 'fahd');
                
                if (isCalling) {
                    setTimeout(() => {
                        startRecording();
                    }, 2000);
                }
            }
        }

        // Start/Stop call
        callButton.addEventListener('click', async () => {
            if (!isCalling) {
                isCalling = true;
                callButton.textContent = 'ğŸ“ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©';
                callButton.disabled = true;
                conversationDiv.innerHTML = '';
                conversationHistory = []; // Reset conversation history
                
                // Stop any playing audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                    currentAudio = null;
                }
                
                // Request microphone access FIRST (must be in user interaction context)
                updateStatus('ğŸ¤ Ø·Ù„Ø¨ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†...', 'processing');
                
                try {
                    // Get microphone stream and keep it open for entire call
                    microphoneStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    
                    // Microphone access granted, proceed with greeting
                    updateStatus('ğŸ”„ Ø¨Ø¯Ø¡ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©...', 'processing');
                    const greeting = 'Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ ÙÙ‡Ø¯ØŒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ';
                    addMessage(greeting, 'fahd');
                    
                    await textToSpeech(greeting);
                    
                    callButton.disabled = false;
                    
                    // Now start recording (using the open stream)
                    startRecording();
                } catch (error) {
                    console.error('Microphone access denied:', error);
                    console.error('Error name:', error.name);
                    console.error('Error message:', error.message);
                    console.error('Error stack:', error.stack);
                    console.error('Full error object:', JSON.stringify(error, Object.getOwnPropertyNames(error)));
                    isCalling = false;
                    callButton.textContent = 'ğŸ“ Ø§ØªØµÙ„ Ø¨ÙÙ‡Ø¯';
                    callButton.disabled = false;
                    
                    let errorMessage = `âŒ Microphone Error\n\nName: ${error.name}\nMessage: ${error.message}`;
                    
                    if (error.name === 'NotAllowedError') {
                        errorMessage += `\n\nâš ï¸ Permission Denied!\n\nPossible causes:\n1. Running in WhatsApp/WebView (not supported)\n2. Permission was denied\n3. Need HTTPS connection\n\nTry opening in Chrome/Safari browser directly`;
                    }
                    
                    errorMessage += `\n\nFull error logged to console`;
                    updateStatus(errorMessage, 'error');
                }
            } else {
                isCalling = false;
                callButton.textContent = 'ğŸ“ Ø§ØªØµÙ„ Ø¨ÙÙ‡Ø¯';
                stopRecording();
                
                // Stop microphone stream
                if (microphoneStream) {
                    microphoneStream.getTracks().forEach(track => track.stop());
                    microphoneStream = null;
                }
                
                // Stop any playing audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                    currentAudio = null;
                }
                
                updateStatus('ØªÙ… Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©', 'idle');
            }
        });

        // Initialize
        initSpeechRecognition();
    </script>
</body>
</html>